{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***COGS 118A FINAL PROJECT:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 13)\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# fetch heart disease dataset\n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "hd_X = heart_disease.data.features\n",
    "hd_Y = heart_disease.data.targets \n",
    "  \n",
    "#According to the repository, the target 0 indicates no presence of heart disease, while any integer 1,2,3, or 4\n",
    "#indicates the presence of heart disease. So, we will simplify this into a binary classification with positive and\n",
    "#negative classes by changing all nonzero targets to +1, and all zero targets to -1.\n",
    "hd_Y = hd_Y.replace(to_replace=0, value=-1)\n",
    "hd_Y = hd_Y.replace(to_replace=[1,2,3,4], value=1)\n",
    "#Our dataset also has missing values. Let's see how many.\n",
    "num_rows_nan = hd_X.isna().sum()\n",
    "#print(num_rows_nan)\n",
    "#Since there are very few nan values, we will just drop those data samples.\n",
    "nan_index = hd_X.loc[~hd_X.index.isin(hd_X.dropna().index)].index\n",
    "hd_X = pd.get_dummies(hd_X)\n",
    "hd_X = hd_X.dropna()\n",
    "hd_Y = hd_Y.drop(index=nan_index)\n",
    "#print(hd_X)\n",
    "\n",
    "#Here, we scale all of our data to be on a normal Gaussian distribution, so they have equal mean and variance. This will ensure\n",
    "#That each variable in our model has equal weight since it is from the same distribution, thus letting our algorithms perform better.\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "hd_X = scaler.fit_transform(X=hd_X)\n",
    "print(hd_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "bc_X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "bc_Y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "num_rows_nan = bc_X.isna().sum()\n",
    "\n",
    "#Since there are very few nan values, we will just drop those data samples.\n",
    "nan_index = bc_X.loc[~bc_X.index.isin(bc_X.dropna().index)].index\n",
    "bc_X = bc_X.dropna()\n",
    "bc_Y = bc_Y.drop(index=nan_index)\n",
    "\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "bc_X = scaler.fit_transform(X=bc_X)\n",
    "print(bc_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(47621, 108)\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "ad_X = adult.data.features \n",
    "ad_Y = adult.data.targets \n",
    "  \n",
    "# metadata \n",
    "#print(adult.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(adult.variables) \n",
    "ad_Y = ad_Y.replace(to_replace=['<=50K', '<=50K.'], value=-1)\n",
    "ad_Y = ad_Y.replace(to_replace=['>50K.', '>50K'], value=1)\n",
    "\n",
    "num_rows_nan = ad_X.isna().sum()\n",
    "\n",
    "#Since there are very few nan values, we will just drop those data samples.\n",
    "nan_index = ad_X.loc[~ad_X.index.isin(ad_X.dropna().index)].index\n",
    "ad_X=pd.DataFrame(ad_X)\n",
    "ad_X = ad_X.dropna()\n",
    "ad_X = pd.get_dummies(ad_X)\n",
    "ad_Y = ad_Y.drop(index=nan_index)\n",
    "\n",
    "#Here, we scale all of our data to be on a normal Gaussian distribution, so they have equal mean and variance. This will ensure\n",
    "#That each variable in our model has equal weight since it is from the same distribution, thus letting our algorithms perform better.\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "ad_X = scaler.fit_transform(X=ad_X)\n",
    "print(ad_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's get our training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ad_X\n",
    "Y = ad_Y\n",
    "test_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'C': 100.0}\n",
      "Validation Accuracy: 0.8504306469100337\n",
      "Training Accuracy: 0.8517954640907182\n",
      "Test Accuracy: 0.8532283464566929\n"
     ]
    }
   ],
   "source": [
    "hd_X_train, hd_X_test, hd_Y_train, hd_Y_test = train_test_split(X, Y, test_size=test_split)\n",
    "\n",
    "#sklearn's Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_classifier = LogisticRegression()\n",
    "param_grid = [{'C': [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1,1e2,1e3,1e4]}]\n",
    "grid_search = GridSearchCV(estimator=logistic_classifier, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X=hd_X_train, y=hd_Y_train)\n",
    "print(\"Best parameters\", grid_search.best_params_)\n",
    "Y_pred = grid_search.predict(hd_X_test)\n",
    "print(\"Validation Accuracy:\", max(grid_search.cv_results_['mean_test_score']))\n",
    "print(\"Training Accuracy:\", grid_search.score(hd_X_train, hd_Y_train))\n",
    "print(\"Test Accuracy:\", grid_search.score(hd_X_test, hd_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'hidden_layer_sizes': (4, 4)}\n",
      "Validation Accuracy: 0.8504567695993234\n",
      "Training Accuracy: 0.8548404031919362\n",
      "Test Accuracy: 0.8548031496062992\n"
     ]
    }
   ],
   "source": [
    "hd_X_train, hd_X_test, hd_Y_train, hd_Y_test = train_test_split(X, Y, test_size=test_split)\n",
    "\n",
    "#sklearn's Artificial Neural Network (ANN)\n",
    "from sklearn.model_selection import cross_validate \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "param_grid = [{'hidden_layer_sizes': [(1,1),(2,2),(4,4),(8,8),(16,16)]}]\n",
    "\n",
    "MLP_classifier = MLPClassifier(random_state=1, solver='sgd')\n",
    "grid_search = GridSearchCV(estimator=MLPClassifier(), param_grid=param_grid, cv=3,\n",
    "                           scoring='accuracy')\n",
    "grid_search.fit(X=hd_X_train, y=hd_Y_train)\n",
    "print(\"Best parameters\", grid_search.best_params_)\n",
    "Y_pred = grid_search.predict(hd_X_test)\n",
    "print(\"Validation Accuracy:\", max(grid_search.cv_results_['mean_test_score']))\n",
    "print(\"Training Accuracy:\", grid_search.score(hd_X_train, hd_Y_train))\n",
    "print(\"Test Accuracy:\", grid_search.score(hd_X_test, hd_Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'n_neighbors': array([    1,  1905,  3810,  5715,  7620,  9525, 11429, 13334, 15239,\n",
      "       17144, 19049, 20953, 22858, 24763, 26668, 28573, 30477, 32382,\n",
      "       34287, 36192, 38097, 40001, 41906, 43811, 45716, 47621])}]\n",
      "Best parameters {'n_neighbors': 1}\n",
      "Validation Accuracy: nan\n",
      "Training Accuracy: 0.9999212515749685\n",
      "Test Accuracy: 0.7865616797900262\n"
     ]
    }
   ],
   "source": [
    "hd_X_train, hd_X_test, hd_Y_train, hd_Y_test = train_test_split(X, Y, test_size=test_split)\n",
    "\n",
    "#sklearn's KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = [{'n_neighbors': np.linspace(start=1,stop=X.shape[0], num=26, dtype=int)}]\n",
    "print(param_grid)\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(estimator=knn_classifier, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X=hd_X_train, y=hd_Y_train)\n",
    "print(\"Best parameters\", grid_search.best_params_)\n",
    "Y_pred = grid_search.predict(hd_X_test.values)\n",
    "print(\"Validation Accuracy:\", max(grid_search.cv_results_['mean_test_score']))\n",
    "print(\"Training Accuracy:\", grid_search.score(X=hd_X_train.values, y=hd_Y_train.values))\n",
    "print(\"Test Accuracy:\", grid_search.score(hd_X_test.values, hd_Y_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'max_depth': 17}\n",
      "Validation Accuracy: 0.863555182540834\n",
      "Training Accuracy: 0.8913271734565309\n",
      "Test Accuracy: 0.853753280839895\n"
     ]
    }
   ],
   "source": [
    "hd_X_train, hd_X_test, hd_Y_train, hd_Y_test = train_test_split(X, Y, test_size=test_split)\n",
    "\n",
    "#sklearn's Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "param_grid = [{'max_depth': np.arange(1,20)}]\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X=hd_X_train, y=hd_Y_train)\n",
    "print(\"Best parameters\", grid_search.best_params_)\n",
    "Y_pred = grid_search.predict(hd_X_test)\n",
    "print(\"Validation Accuracy:\", max(grid_search.cv_results_['mean_test_score']))\n",
    "print(\"Training Accuracy:\", grid_search.score(hd_X_train, hd_Y_train))\n",
    "print(\"Test Accuracy:\", grid_search.score(hd_X_test, hd_Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'max_depth': 9}\n",
      "DT Validation Accuracy: 0.8540004156560791\n",
      "DT Training Accuracy: 0.8605365392692146\n",
      "TEST ACCURACY: 0.8558530183727034\n"
     ]
    }
   ],
   "source": [
    "hd_X_train, hd_X_test, hd_Y_train, hd_Y_test = train_test_split(X, Y, test_size=test_split)\n",
    "\n",
    "#sklearn's Decision Tree\n",
    "tree_classifier = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "D_list = np.arange(1,20)\n",
    "param_grid = {'max_depth': D_list}\n",
    "grid_search = GridSearchCV(estimator=tree_classifier,param_grid=param_grid,cv=3)\n",
    "grid_search.fit(X=hd_X_train, y=hd_Y_train)\n",
    "print(\"Best parameters\", grid_search.best_params_)\n",
    "Y_pred = grid_search.predict(hd_X_test)\n",
    "print(\"DT Validation Accuracy:\", max(grid_search.cv_results_['mean_test_score']))\n",
    "print(\"DT Training Accuracy:\", grid_search.score(hd_X_train, hd_Y_train))\n",
    "print(\"TEST ACCURACY:\", grid_search.score(hd_X_test, hd_Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracies(estimators, param_grids, test_splits, X, Y):\n",
    "    for test_split in test_splits:\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=test_split)\n",
    "        print(\"***TEST PROPORTION:***\", test_split)\n",
    "        print()\n",
    "        i = 0\n",
    "        for estimator, param_grid in zip(estimators, param_grids):\n",
    "            if i==0:\n",
    "                print(\"DECISION TREE\")\n",
    "            elif i==1:\n",
    "                print(\"RANDOM FOREST\")\n",
    "            elif(i==2):\n",
    "                print(\"LOGISTIC REGRESSION\")\n",
    "            elif(i==3):\n",
    "                print(\"ANN\")\n",
    "            elif(i==4):\n",
    "                print(\"KNN\")\n",
    "            grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3)\n",
    "            grid_search.fit(X=train_X.values, y=train_Y.values)\n",
    "            print(\"TRAINING ACCURACY:\", grid_search.score(train_X.values, train_Y.values))\n",
    "            print(\"VALIDATION ACCURACY:\", max(grid_search.cv_results_['mean_test_score']))\n",
    "            print(\"TEST ACCURACY:\", grid_search.score(test_X.values, test_Y.values))\n",
    "            print(\"BEST PARAMETERS:\", grid_search.best_params_)\n",
    "            print()\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TEST PROPORTION:*** 0.2\n",
      "\n",
      "DECISION TREE\n",
      "TRAINING ACCURACY: 0.9758241758241758\n",
      "VALIDATION ACCURACY: 0.9297083769025211\n",
      "TEST ACCURACY: 0.9385964912280702\n",
      "BEST PARAMETERS: {'max_depth': 3}\n",
      "\n",
      "RANDOM FOREST\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9560822586266992\n",
      "TEST ACCURACY: 0.9473684210526315\n",
      "BEST PARAMETERS: {'max_depth': 17}\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "TRAINING ACCURACY: 0.9868131868131869\n",
      "VALIDATION ACCURACY: 0.9802341117694899\n",
      "TEST ACCURACY: 0.9736842105263158\n",
      "BEST PARAMETERS: {'C': 1}\n",
      "\n",
      "ANN\n",
      "TRAINING ACCURACY: 0.9934065934065934\n",
      "VALIDATION ACCURACY: 0.9758481468572092\n",
      "TEST ACCURACY: 0.9736842105263158\n",
      "BEST PARAMETERS: {'hidden_layer_sizes': (16, 16)}\n",
      "\n",
      "KNN\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9604537004763566\n",
      "TEST ACCURACY: 0.9298245614035088\n",
      "BEST PARAMETERS: {'n_neighbors': 1}\n",
      "\n",
      "***TEST PROPORTION:*** 0.5\n",
      "\n",
      "DECISION TREE\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9436356849570734\n",
      "TEST ACCURACY: 0.9192982456140351\n",
      "BEST PARAMETERS: {'max_depth': 13}\n",
      "\n",
      "RANDOM FOREST\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9752519596864501\n",
      "TEST ACCURACY: 0.9578947368421052\n",
      "BEST PARAMETERS: {'max_depth': 10}\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "TRAINING ACCURACY: 0.9894366197183099\n",
      "VALIDATION ACCURACY: 0.9682717431877567\n",
      "TEST ACCURACY: 0.9824561403508771\n",
      "BEST PARAMETERS: {'C': 1}\n",
      "\n",
      "ANN\n",
      "TRAINING ACCURACY: 0.9964788732394366\n",
      "VALIDATION ACCURACY: 0.9647629712579321\n",
      "TEST ACCURACY: 0.9578947368421052\n",
      "BEST PARAMETERS: {'hidden_layer_sizes': (16, 16)}\n",
      "\n",
      "KNN\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9647629712579321\n",
      "TEST ACCURACY: 0.9508771929824561\n",
      "BEST PARAMETERS: {'n_neighbors': 1}\n",
      "\n",
      "***TEST PROPORTION:*** 0.8\n",
      "\n",
      "DECISION TREE\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.95542911332385\n",
      "TEST ACCURACY: 0.918859649122807\n",
      "BEST PARAMETERS: {'max_depth': 3}\n",
      "\n",
      "RANDOM FOREST\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9734471313418682\n",
      "TEST ACCURACY: 0.9320175438596491\n",
      "BEST PARAMETERS: {'max_depth': 4}\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9556661925082978\n",
      "TEST ACCURACY: 0.9736842105263158\n",
      "BEST PARAMETERS: {'C': 1}\n",
      "\n",
      "ANN\n",
      "TRAINING ACCURACY: 0.9911504424778761\n",
      "VALIDATION ACCURACY: 0.9644381223328592\n",
      "TEST ACCURACY: 0.9517543859649122\n",
      "BEST PARAMETERS: {'hidden_layer_sizes': (8, 8)}\n",
      "\n",
      "KNN\n",
      "TRAINING ACCURACY: 1.0\n",
      "VALIDATION ACCURACY: 0.9468942626837364\n",
      "TEST ACCURACY: 0.9495614035087719\n",
      "BEST PARAMETERS: {'n_neighbors': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimators = [tree.DecisionTreeClassifier(), RandomForestClassifier(), LogisticRegression(), MLPClassifier(),KNeighborsClassifier()]\n",
    "test_splits = [0.2, 0.5, 0.8]\n",
    "param_grids = []\n",
    "param_grids.append({'max_depth': np.arange(1,20)})\n",
    "param_grids.append({'max_depth': np.arange(1,20)})\n",
    "param_grids.append({'C': [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1,1e2,1e3,1e4]})\n",
    "param_grids.append({'hidden_layer_sizes': [(1,1),(2,2),(4,4),(8,8),(16,16)]})\n",
    "param_grids.append({'n_neighbors': np.linspace(start=1,stop=X.shape[0], num=26, dtype=int)})\n",
    "\n",
    "find_accuracies(estimators=estimators, param_grids=param_grids, test_splits=test_splits, X=bc_X, Y=bc_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [tree.DecisionTreeClassifier(), RandomForestClassifier(), LogisticRegression(), MLPClassifier(),KNeighborsClassifier()]\n",
    "test_splits = [0.2, 0.5, 0.8]\n",
    "param_grids = []\n",
    "param_grids.append({'max_depth': np.arange(1,20)})\n",
    "param_grids.append({'max_depth': np.arange(1,20)})\n",
    "param_grids.append({'C': [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1,1e2,1e3,1e4]})\n",
    "param_grids.append({'hidden_layer_sizes': [(1,1),(2,2),(4,4),(8,8),(16,16)]})\n",
    "param_grids.append({'n_neighbors': np.linspace(start=1,stop=X.shape[0], num=26, dtype=int)})\n",
    "\n",
    "find_accuracies(estimators=estimators, param_grids=param_grids, test_splits=test_splits, X=hd_X, Y=hd_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [tree.DecisionTreeClassifier(), RandomForestClassifier(), LogisticRegression(), MLPClassifier(),KNeighborsClassifier()]\n",
    "test_splits = [0.2, 0.5, 0.8]\n",
    "param_grids = []\n",
    "param_grids.append({'max_depth': np.arange(1,20)})\n",
    "param_grids.append({'max_depth': np.arange(1,20)})\n",
    "param_grids.append({'C': [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1,1e2,1e3,1e4]})\n",
    "param_grids.append({'hidden_layer_sizes': [(1,1),(2,2),(4,4),(8,8),(16,16)]})\n",
    "param_grids.append({'n_neighbors': np.linspace(start=1,stop=X.shape[0], num=26, dtype=int)})\n",
    "\n",
    "find_accuracies(estimators=estimators, param_grids=param_grids, test_splits=test_splits, X=bc_X, Y=bc_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
